{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generar datos de ejemplo\n",
    "n_features = 10\n",
    "X,y = make_classification(\n",
    "  n_samples=100, \n",
    "  n_features=n_features, \n",
    "  n_informative=5, \n",
    "  n_classes=3, \n",
    "  random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame para mejorar visualización\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i+1}\" for i in range(n_features)])\n",
    "df['Class'] = y \n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623fd44",
   "metadata": {},
   "source": [
    "### Visualización de Características en Espacio de Búsqueda\n",
    "\n",
    "Cuando se tiene muchas características (dimensiones), es difícil visualizarlas directamente. Para resolver esto, se usan técnicas como:\n",
    "- *Reducción de Dimensionalidad*: Proyectan datos $N$-dimensionales a $2/3$-dimensionales.\n",
    "- *Muestreo de Relaciones*: Crear matrices de correlación o gráficos paralelos.\n",
    "- *Clustering y Visualización*: Aplicar algoritmos de clustering y visualizar los resultados según categorías o valores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e6961",
   "metadata": {},
   "source": [
    "#### Matriz de Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83845ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Matriz de Correlación entre Características')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20201d09",
   "metadata": {},
   "source": [
    "#### Gráfico de Tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c30397",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 4\n",
    "sns.pairplot(df, vars=df.columns[:LIMIT], hue='Class', palette='viridis')\n",
    "plt.suptitle('Gráfico de Pares de Características', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1ad0b",
   "metadata": {},
   "source": [
    "#### Coordenadas Paralelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 5\n",
    "plt.figure(figsize=(12, 6))\n",
    "pd.plotting.parallel_coordinates(df, 'Class', cols=df.columns[:LIMIT], colormap='viridis')\n",
    "plt.title('Coordenadas Paralelas de Características')\n",
    "plt.xlabel('Características')\n",
    "plt.ylabel('Valores normalizados')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f383f7",
   "metadata": {},
   "source": [
    "### Aplicación de PCA y Autoencoders para Visualización\n",
    "\n",
    "PCA y Autoencoders son técnicas de reducción de dimensionalidad: \n",
    "- **PCA**: Encuentra las direcciones de máxima varianza (componentes principales).\n",
    "- **Autoencoders**: Red Neuronal que comprime y reconstruye datos, aprendiendo representaciones compactas.\n",
    "\n",
    "Ambos permiten visualizar datos complejos en 2/3-dimensionales manteniendo la estructura subyacente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X,y = make_classification(\n",
    "  n_samples=500, \n",
    "  n_features=20, \n",
    "  n_informative=10, \n",
    "  n_classes=4,\n",
    "  random_state=42\n",
    ")\n",
    "# Estandarizar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc85b63",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "**Objetivo**: Encontrar una *transformación ortogonal* que convierta un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables linealmente no correlacionadas, llamadas *componentes principales*. El primer componente principal tiene la varianza más alta posible, y cada componente sucesivo, a su vez, tiene la más alta alta varianza posible bajo la restricción de que es ortogonal a los componentes anteriores\n",
    "\n",
    "--- \n",
    "\n",
    "1. *Centrar datos*: $\\hat{X} = X - \\mu$\n",
    "2. *Matriz de covarianza*: $C = 1/m \\hat{X}^T \\hat{X}$\n",
    "3. *Descomposición espectral*: $C = V \\Lambda V^T$\n",
    "4. *Proyección*: $Z = \\hat{X} V_k$ donde $V_k$ son los $k$ primeros autovectores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Varianza explicada: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Varianza total explicada: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Visualización PCA 2D\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k')\n",
    "plt.colorbar(scatter, label='Clase')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "plt.title('PCA: Proyección 2D de 20 características')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05af021",
   "metadata": {},
   "source": [
    "##### Visualización: PCA 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter3D(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=y, cmap='viridis', alpha=0.7, s=50)\n",
    "plt.colorbar(scatter, label='Class')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA 3D - Varianza: ' + f'{sum(pca_3d.explained_variance_ratio_):.1%}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6411213",
   "metadata": {},
   "source": [
    "#### Autoencoders\n",
    "\n",
    "**Descripción**: Red Neuronal que se entrena para reconstruir su entrada. Está compuesto por dos funciones parametrizadas: \n",
    "- **Encoder** (**Codificador**): Una función $f_{\\phi}$ que transforma un vector de entrada $\\textbf{f} \\in \\mathbb{R}^d$ en una representación latente $\\mathbf{z} \\in \\mathbb{R}^m$\n",
    "- **Decoder** (**Decodificador**): Una función $g_\\theta$ que mapea el código latente $\\mathbf{z}$ de vuelta al espacio original, produciendo una reconstrucción $x' \\in \\mathbb{R}^d$  \n",
    "\n",
    "---\n",
    "\n",
    "Se puede expresar de la siguiente forma:\n",
    "$$\\begin{matrix}\n",
    "\\mathbf{z} = f_{\\phi}(\\mathbf{x}) \\\\\n",
    "\\mathbf{x}' = g_{\\theta}(\\mathbf{z}) = g_{\\theta}(f_{\\theta}(\\mathbf{x}))\n",
    "\\end{matrix}$$\n",
    "donde:\n",
    "- $\\phi:$ son los parámetros (pesos y sesgos) del codificador  \n",
    "- $\\theta:$ son los parámetros del decodificador\n",
    "\n",
    "--- \n",
    "\n",
    "**Espacio Latente** (**Bottleneck**): La dimensionalidad del código latente $m$ es crucial. Típicamente, se fuerza a que $m < d$ (subespacio), lo que obliga a la red a aprender una comprensión con pérdidas de los datos más informativa. Sin embargo, también existen variantes de autoencoders con $m > d$ que requieren fuertes regularizaciones para ser útiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración: 20 -> 10 -> 2 -> 10 -> 20\n",
    "autoencoder = MLPRegressor(\n",
    "  # Capa bottleneck de 2 dimensiones\n",
    "  hidden_layer_sizes=(10, 2, 10),\n",
    "  activation='relu',\n",
    "  solver='adam',\n",
    "  max_iter=1000,\n",
    "  random_state=42,\n",
    "  verbose=False\n",
    ")\n",
    "\n",
    "# Entrenar autoencoder\n",
    "autoencoder.fit(X_scaled, X_scaled)\n",
    "\n",
    "# Extraer la representación intermedia (2D)\n",
    "# Necesitamos acceder a la capa de 2 dimensiones\n",
    "# Para simplificar, crearemos un encoder manual:\n",
    "class SimpleAutoencoder:\n",
    "  def __init__(self):\n",
    "    self.encoder = MLPRegressor(\n",
    "      hidden_layer_sizes=(10, 2),\n",
    "      activation='relu',\n",
    "      solver='adam',\n",
    "      max_iter=1000,\n",
    "      random_state=42\n",
    "    )\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    # Entrenar para aprender representación\n",
    "    self.encoder.fit(X, X[:, :2])  # Aprender a comprimir a 2D\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return self.encoder.predict(X)\n",
    "\n",
    "# Crear y entrenar autoencoder simplificado\n",
    "ae = SimpleAutoencoder()\n",
    "ae.fit(X_scaled)\n",
    "X_ae = ae.transform(X_scaled)\n",
    "\n",
    "# Visualización Autoencoder\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_ae[:, 0], X_ae[:, 1], c=y, cmap='plasma', alpha=0.7, edgecolors='k')\n",
    "plt.colorbar(scatter, label='Clase')\n",
    "plt.xlabel('Dimensión Latente 1')\n",
    "plt.ylabel('Dimensión Latente 2')\n",
    "plt.title('Autoencoder: Representación Latente 2D')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaaf17",
   "metadata": {},
   "source": [
    "#### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71808f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# PCA\n",
    "sc1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "axes[0].set_title(f'PCA (Varianza: {sum(pca.explained_variance_ratio_):.1%})')\n",
    "axes[0].set_xlabel('Componente 1')\n",
    "axes[0].set_ylabel('Componente 2')\n",
    "plt.colorbar(sc1, ax=axes[0])\n",
    "\n",
    "# Autoencoder\n",
    "sc2 = axes[1].scatter(X_ae[:, 0], X_ae[:, 1], c=y, cmap='plasma', alpha=0.7)\n",
    "axes[1].set_title('Autoencoder (2D)')\n",
    "axes[1].set_xlabel('Dimensión 1')\n",
    "axes[1].set_ylabel('Dimensión 2')\n",
    "plt.colorbar(sc2, ax=axes[1])\n",
    "\n",
    "# t-SNE para comparación (no lineal)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "sc3 = axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "axes[2].set_title('t-SNE (Referencia no lineal)')\n",
    "axes[2].set_xlabel('Dimensión 1')\n",
    "axes[2].set_ylabel('Dimensión 2')\n",
    "plt.colorbar(sc3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
