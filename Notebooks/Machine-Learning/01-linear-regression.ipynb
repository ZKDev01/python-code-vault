{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar los datos y las librerías\n",
    "\n",
    "> **¿Qué es un archivo `csv`?**\n",
    "> \n",
    "> Un archivo csv (Comma-Separated Values) es un tipo de archivo de texto plano que almacena datos tabulares, es decir, datos dispuestos en filas y columnas. Cada fila en el archivo representa un registro o una fila de datos, y cada columna representa un campo o característica de los datos. Los valores en cada fila están separados por comas, de ahí el nombre \"Comma-Separated Values\". Este formato es ampliamente utilizado para el intercambio de datos entre diferentes sistemas y aplicaciones debido a su simplicidad y compatibilidad con una amplia gama de software, incluyendo hojas de cálculo como Microsoft Excel y programas de análisis de datos como Python con la biblioteca Pandas.\n",
    "> \n",
    "> Los archivos csv son especialmente útiles en el campo de la ciencia de datos y el análisis de datos porque permiten una fácil importación y exportación de datos para su procesamiento y análisis. Por ejemplo, en Python, la biblioteca Pandas proporciona la función read_csv() para leer archivos csv y convertirlos en DataFrames, que son estructuras de datos bidimensionales que permiten el manejo y análisis de datos de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv( 'database/house-prices-train.csv' )\n",
    "testset = pd.read_csv( 'database/house-prices-test.csv' )\n",
    "\n",
    "trainset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Función `describe()` de `pandas`**\n",
    ">\n",
    "> Genera estadísticas descriptivas de las columnas numéricas en un DataFrame. Cuando se aplica a un DataFrame que se ha creado a partir de un archivo CSV utilizando `read_csv()`, proporciona un resumen estadístico de las columnas numéricas, incluyendo el conteo, la media, la desviación estándar, los valores mínimos y máximos, y los cuartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación del resultado**\n",
    "\n",
    "`count` $\\to$ significa la cantidad de datos \n",
    "\n",
    "`mean` $\\to$ media\n",
    "\n",
    "`std` $\\to$ desviación estándar\n",
    "\n",
    "`min` $\\to$ valor mínimo\n",
    "\n",
    "`max` $\\to$ valor máximo\n",
    "\n",
    "`25%` $\\to$ valor de la posición del primer cuartil \n",
    "\n",
    "`50%` $\\to$ valor de la posición del segundo cuartil o mediana \n",
    "\n",
    "`75%` $\\to$ valor de la posición del tercer cuartil "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuartiles y Percentiles\n",
    "Se utilizan para describir la posición de un dato específico con respecto al resto de los datos cuando están en orden por categorías\n",
    "\n",
    "- El primer cuartil es el valor por debajo del cual se encuentra el 25% de los datos. Indica el valor más bajo que no es menor que el 25% de los datos. \n",
    "- El segundo cuartil o mediana es el valor central de un conjunto de datos\n",
    "- El tercer cuartil es el valor por debajo del cual se encuentra el 75% de los datos. Específicamente, indica el valor más bajo que no es menor que el 75% de los datos\n",
    "\n",
    "Los percentiles son puntos que dividen un conjunto de datos en 100 partes iguales. Cada percentil representa el valor por debajo del cual se encuentra un porcentaje específico de los datos. \n",
    "\n",
    "> Para calcular el percentil es ordenar los valores de la tabla y coger el valor que se encuentra en la posición que cae al calcular el porciento\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desviación Estándar\n",
    "Es un número que describe cuán dispersas están las observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std( trainset[ ['SalePrice'] ], axis=0 )\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente de Variación\n",
    "Se utiliza para tener una idea de qué tan grande es la desviación estándar. El coeficiente de variación se expresa en porcentaje y proporciona una medida de cuánto varína los datos en una relación con su media. \n",
    "\n",
    "Este se calcula con: $cv = std/mean$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.std( trainset[ ['SalePrice'] ], axis=0 ) / np.mean( trainset[ ['SalePrice'] ] )\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varianza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[ ['GrLivArea', 'SalePrice'] ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.plot.scatter( x='GrLivArea', y='SalePrice' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta gráfica de datos podría definirse como un primer modelo de regresión (simple). En esta se puede trazar una recta y definir que existe un error. El error viene dado por la diferencia del valor de la recta y el valor correcto. Tomando la suma de estos errores y dividiendolo por la cantidad de errores existentes tenemos el *error cuadrático medio* (ECM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de la recta\n",
    "w = 120\n",
    "b = 0\n",
    "\n",
    "# Puntos de la recta \n",
    "x = np.linspace(0, trainset['GrLivArea'].max(), 100)\n",
    "y = w*x + b\n",
    "\n",
    "# Gráfica de la recta\n",
    "trainset.plot.scatter(x='GrLivArea', y='SalePrice')\n",
    "plt.plot(x, y, '-r')\n",
    "plt.ylim(0, trainset['SalePrice'].max()*1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de Predicciones (1er)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si escogemos esos parametros para el modelo cuál es el error? \n",
    "\n",
    "# calculo de las predicciones\n",
    "trainset['pred'] = trainset['GrLivArea']*w+b\n",
    "\n",
    "# calculo de la funcion de error \n",
    "trainset['diff'] = trainset['pred']-trainset['SalePrice']\n",
    "trainset['cuad'] = trainset['diff']**2\n",
    "trainset[ ['GrLivArea', 'SalePrice', 'pred', 'diff', 'cuad'] ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando sklearn para el 2do cálculo de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo input y output\n",
    "X_train = np.array(trainset[ 'GrLivArea' ]).reshape((-1, 1))\n",
    "Y_train = np.array(trainset[ 'SalePrice' ])\n",
    "\n",
    "# creando modelo\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "intercept = model.intercept_\n",
    "coef = model.coef_\n",
    "\n",
    "# imprimiendo parametros\n",
    "print(f\"intercepto (b): {intercept}\")\n",
    "print(f\"pendiente (w): {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, trainset['GrLivArea'].max(), 100)\n",
    "y = coef*x + intercept\n",
    "\n",
    "# Gráfica de la recta\n",
    "trainset.plot.scatter(x='GrLivArea', y='SalePrice')\n",
    "plt.plot(x, y, '-r')\n",
    "plt.ylim(0, trainset['SalePrice'].max()*1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAG\n",
    "\n",
    "### Regresión Lineal\n",
    "\n",
    "Este se usa para intentar encontrar la relación entre las variables. La línea que se obtiene a través de la regresión lineal se llama línea de regresión. La idea es que esta línea representa la relación entre las variables de manera que la suma de las diferencias entre los valores observados y los valores predichos por la línea de regresión sea lo más chica posible\n",
    "\n",
    "### Método de los mínimos cuadrados\n",
    "Este método es utilizado para encontrar la mejor línea de ajuste (o modelo) que se ajuste a un conjunto de datos. Este método es método en regresión lineal se utiliza para minimizar la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por el modelo  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los Supuestos\n",
    "\n",
    "### Supuesto de Homocedasticidad \n",
    "\n",
    "La prueba de Breusch-Pagan es una forma de comprobar si existe heterocedasticidad en el análisis de regresión. Una prueba de Breusch-Pagan sigue las siguientes hipótesis:\n",
    "- **Hipótesis nula:** Significa que la homocedasticidad está presente.\n",
    "- **Hipótesis alternativa:** Significa que la homocedasticidad no está presente (es decir, existe heterocedasticidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un conjunto de datos\n",
    "dataframe = pd.DataFrame({\n",
    "  'rating': [92, 84, 87, 82, 98, 94, 75, 80, 83, 89],\n",
    "  'points': [27, 30, 15, 26,27, 20, 16, 18,19, 20],\n",
    "  'runs': [5000, 7000, 5102, 8019, 1200, 7210, 6200, 9214, 4012, 3102],\n",
    "  'wickets': [110, 120, 110, 80, 90, 119, 116, 100, 90, 76]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo de regresión lineal múltiple\n",
    "fit = smf.ols('rating ~ points + runs + wickets', data=dataframe).fit()\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Lagrange multiplier statistic', 'p-value','f-value', 'f p-value']\n",
    "# Obtener resultados del test\n",
    "test_result = sms.het_breuschpagan(fit.resid, fit.model.exog)\n",
    "lzip(names, test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el valor $p$ es mayor que 0.05, no podemos rechazar la hipótesis nula. Por tanto, no tenemos pruebas suficientes para decir que la heteroscedasticidad está presente en el modelo de regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Información de la Tabla de Regresión\n",
    "##### Básicos\n",
    "- `Dep. Variable` $\\to$ Sería la variable dependiente. En la tabla sería `Calorie_Burnage`. La variable dependiente se explica aquí mediante `Average_Pulse`\n",
    "- `Model` $\\to$ OLS es Ordinary Least Squares. \n",
    "- `Date` and `Time` $\\to$ Muestra la fecha y la hora de salida que fue calculado en Python\n",
    "\n",
    "##### Seción de Coeficientes\n",
    "- `coef` $\\to$ es la abreviatura de coeficiente. Están las salidas de la función de la regresión lineal \n",
    "- `Intercept` y `Average_Pulse` en `coef` $\\to$ hace que la función de regresión lineal pueda reescribirse de la siguiente forma: \n",
    "$$Calorie\\_Burnage = 0.3296 * Average\\_Pulse + 348.8662$$ \n",
    "Esos números significan que si Average_Pulse se incrementa en 1, Calorie_Burnage se incrementa en 0.3296 (0.3 redondeado). Si Average_Pulse = 0, the Calorie_burnage es igual a 346.8662 (346.9 redondeado). \n",
    "- `Intercept` es usada para ajustar el modelo de precisión de la predición \n",
    "\n",
    "##### Seción de las Estadísticas de los Coeficientes \n",
    "Esta seción sirve para testear si los componentes de la función de regresión lineal tiene un impacto significativo en la variable dependiente (`Calorie_Burnage`). En otras palabras, probar si existe una relación entre `Average_Pulse` y `Calorie_Burnage`, usando las pruebas de estadística\n",
    "\n",
    "**Existen 4 componentes que explican las estadísticas de los coeficientes**\n",
    "- `str err` que sería el error estándar \n",
    "- `t` que sería el valor `t` del coeficiente \n",
    "- `P>|t|` que sería el valor `P`\n",
    "- `[0.025 0.975]` representa el intervalo de confianza de los coeficientes\n",
    "\n",
    "##### El valor `P`\n",
    "El valor `P` es el valor `p` asociado con el coeficiente de una variable independiente en el modelo. Este valor `p` es el resultado de una prueba de hipótesis que se utiliza para determinar si el coeficiente de la variable independiente es significativamente diferente de 0. El valor `P` ayuda a responder la pregunta de si la variable independiente tiene un efecto significativo sobre la variable dependiente. \n",
    "\n",
    "Este valor se calcula utilizando los residuos del modelo y los coeficientes estimados. Un valor `p` bajo ($< 0.05$) indica que es muy poco probable que el coeficiente observado se deba al azar, lo que sugiere que hay una relación significativa entre la variable independiente y la variable dependiente. \n",
    "\n",
    "Un valor `p` alto ($> 0.05$) indica que no hay suficiente evidencia para rechazar la hipótesis nula de que el coeficiente es igual a cero, lo que sugiere que la variable independiente no tiene un efecto significativo sobre la variable dependiente \n",
    "\n",
    "> Si el valor `P` es alto también puede llamarse valor `P` insignificante\n",
    "\n",
    "##### `R-squared` y `Adj. R-squared`\n",
    "`R-squared` es una medida estadística que indica la proporción de la variación en la variable dependiente que es explicada por la(s) variable(s) independiente(s) en el modelo. Es el coeficiente de determinación, que mide qué tan bien el modelo de regresión lineal se ajusta a los datos. \n",
    "\n",
    "Este valor varía entre 0 y 1, donde: \n",
    "- El valor de 0 indica que el modelo no explica ninguna de las variaciones en la variable dependiente \n",
    "- El valor de 1 indica que el modelo explica todas las variaciones en la variable dependiente \n",
    "- Un valor entre 0 y 1 indica la proporción de la variación dependiente que es explicada por el modelo\n",
    "\n",
    "**Ejemplo:** un valor de $0.85$ significa que el 85% de la variación en la variable dependiente puede ser explicada por las variables independientes en el modelo \n",
    "\n",
    "> Que el modelo de regresión lineal no explica ninguna de las variaciones en la variable dependiente, significa que el modelo de regresión lineal no puede predecir o explicar ningún cambio o variación en los valores de la variable dependiente basándose en los valores de las variables independientes. El modelo no tiene ninguna relación entre las variables independientes y la variable dependiente \n",
    "\n",
    "Que el valor sea 0 puede ser resultado de varias situaciones: \n",
    "- *Variables independientes irrelevantes:* las variables independientes seleccionadas no tienen ninguna relación con la variable dependiente. Esto podría deberse a que las variables independientes no son predictivas de la variable dependiente o que no se han seleccionado adecuadamente \n",
    "- *Errores en la recopilación de datos:* puede haber errores en la recopilación de datos que hacen que las variables independientes no estén relacionadas con la variable dependiente \n",
    "- *Modelo incorrecto:* el modelo de regresión lineal no es el más adecuado para los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo OLS \n",
    "OLS significa Ordinary Least Squares (Mínimos Cuadrados Ordinarios). En el contexto de una tabla de regresión lineal, el modelo OLS es un método estadístico utilizado para estimar los parámetros de una regresión lineal, como la pendiente y la intersección en el eje `y`, basándose en el conjunto de datos observado. \n",
    "\n",
    "El objetivo del modelo es minimizar la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por el modelo. Esto se logra encontrando la línea que mejor se ajusta a los datos, es decir, la línea que minimiza la distancia total entre los puntos de datos y la línea \n",
    "\n",
    "### Otros modelos\n",
    "**Regresión de mínimos cuadrados generalizados (GLS):** este modelo es una extensión del modelo OLS que permite manejar errores con varianzas diferentes y autocorrelacionados. Para hacer esto se ajusta una matriz de pesos a los residuos del modelo OLS, lo que resulta en una estimación más precisa de los parámetros \n",
    "\n",
    "**Regresión de mínimos cuadrados robustos (RMS):** modelo similar a OLS, pero utiliza una técnica de estimación de parámetros que es robusta a los errores autocorrelacionados y errores con varianzas diferentes\n",
    "\n",
    "**Regresión de mínimos cuadrados de mínimos residuos (MMR):** modelo que busca minimizar no solo la suma de los cuadrados de los residuos, sino también la suma de los cuadrados de los residuos ponderados por el inverso de la varianza de los errores. Esto puede ser útil cuando los errores tienen una varianza que varía con los valores de `x`\n",
    "\n",
    "**Regresión de mínimos cuadrados de mínimos residuos con ponderación (WMMR):** similar al MMR, pero utiliza una ponderación adicional para los residuos, lo que mejora la precisión de la estimación de los parámetros en ciertos casos \n",
    "\n",
    "**Regresión de mínimos cuadrados de mínimos residuos con ponderación de peso (WWMMR):** modelo, que es una extensión del WMMR, que utiliza una ponderación adicional basada en el peso de los residuos, lo que puede mejorar aún más la precisión de la estimación de los parámetros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supuesto de Independencia\n",
    "Tenemos un dataset que describe 10 jugadores de baloncesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando un conjunto de datos\n",
    "df = pd.DataFrame({\n",
    "    'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],\n",
    "    'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],\n",
    "    'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],\n",
    "    'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que ajustamos un modelo de regresión lineal múltiple utilizando la `rating` como variable dependiente y las otras 3 como variables independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit multiple linear regression model\n",
    "model = ols('rating ~ points + assists + rebounds', data=df).fit()\n",
    "\n",
    "#view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos aplicar la prueba de Durbin-Watson que trae la biblioteca `statsmodels` para determinar si los residuos del modelo de regresión están autocorrelacionados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform Durbin-Watson test\n",
    "durbin_watson(model.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estadístico de la prueba es 2.392 y dado que está dentro del rango de 1.5 y 2.5 consideraríamos que la autocorrelación no es problemática en este modelo de regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supuesto de Linealidad en parámetros: Prueba de RESET de Ramsey "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.diagnostic as smd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houseprices = sm.datasets.get_rdataset(dataname=\"HousePrices\", package=\"AER\", cache=True).data\n",
    "print(houseprices.iloc[:, 0:3].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo `ols` utilizando variables dentro del objeto de datos `houseprices` y almacenamos los resultados dentro del objeto `mlr`. Dentro `ols`, la fórmula del parámetro se ajusta al modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = smf.ols(formula=\"price ~ lotsize + bedrooms\", data=houseprices).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hacemos la prueba RESET de Ramsey "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resettest = smd.linear_reset(res=mlr, power=2, test_type=\"fitted\", use_f=True)\n",
    "print(resettest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supuesto de Normalidad: Kolmogorov-Smirnov\n",
    "\n",
    "Esta prueba determina si dos muestras son significativamente diferentes entre sí. \n",
    "\n",
    "El estadístico de Kolmogorov-Smirnov cuantifica una distancia entre la función de distribución empírica de la muestra y la función acumulativa de la distribución de referencia, o entre las funciones de distribución empírica de dos muestras\n",
    "\n",
    "La hipótesis nula supone que los números están distribuidos uniformemente entre 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "import random\n",
    "\n",
    "# N = int(input(\"Enter number of random numbers: \"))\n",
    "N = 5\n",
    "\n",
    "actual =[]\n",
    "print(\"Enter outcomes: \")\n",
    "for i in range(N):\n",
    "\t# x = float(input(\"Outcomes of class \"+str(i + 1)+\": \"))\n",
    "\tactual.append(random.random())\n",
    "\n",
    "print(actual)\n",
    "x = kstest(actual, \"uniform\") \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KS Test** es una forma de diferenciar automáticamente muestras de una distribución diferente. La función también se puede utilizar para comprobar si los datos proporcionados siguen la distribución normal o no. Por lo tanto, la hipótesis nula supone que los números siguen la distribución normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "actual =[]\n",
    "print(\"Enter outcomes: \")\n",
    "\n",
    "for i in range(N):\n",
    "\t# x = float(input(\"Outcomes of class \"+str(i + 1)+\": \"))\n",
    "\tactual.append(random.random())\n",
    "\n",
    "print(actual)\n",
    "x = kstest(actual, \"norm\") \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supuesto de Normalidad: Shapiro-Wilk\n",
    "\n",
    "El método `shapiro` tiene como parámetros\n",
    "- `x`: Matrix de datos de muestra \n",
    "\n",
    "Retorna los siguientes valores:\n",
    "- `estadística`: Estadística de prueba \n",
    "- `valor p`: El valor $p$ para la prueba de hipótesis\n",
    "\n",
    "La hipótesis nula dice: *la muestra proviene de distribuciones normales*\n",
    "\n",
    "En este ejemplo se va a realizar una prueba de Shapiro-Wilk en los datos generados aleatoriamente con 500 puntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful library\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro\n",
    "from numpy.random import randn\n",
    "\n",
    "# Create data\n",
    "gfg_data = randn(500)\n",
    "\n",
    "# conduct the Shapiro-Wilk Test\n",
    "shapiro(gfg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que en el ejemplo anterior, el valor $p$ es 0.98, que es mayor que el umbral (0.05), entonces no podemos rechazar la hipótesis nula, es decir, no tenemos evidencia suficiente para decir que la muestra no viene de una distribución normal \n",
    "\n",
    "Realizar una prueba de Shapiro-Wilk a datos generados aleatoriamente a partir de los datos de distribución de Poisson con 100 puntos de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful library\n",
    "import numpy as np\n",
    "from numpy.random import poisson\n",
    "from numpy.random import seed\n",
    "from scipy.stats import shapiro\n",
    "from numpy.random import randn\n",
    "\n",
    "seed(0)\n",
    "# Create data\n",
    "gfg_data = poisson(5, 200)\n",
    "\n",
    "# conduct the Shapiro-Wilk Test\n",
    "shapiro(gfg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que en el ejemplo anterior, el valor $p$ es 0.0001 rechazamos la hipótesis nula, es decir, tenemos evidencia suficiente para decir que la muestra no proviene de una distribución normal "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
